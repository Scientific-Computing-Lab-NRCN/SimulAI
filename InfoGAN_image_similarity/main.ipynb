{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from glob import glob\n",
    "from helpers import merge, count_params, cache_result\n",
    "from random import randint\n",
    "from zap50k import zap_data, IMAGE_SIZE\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.misc\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "slim = tf.contrib.slim\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TINY = 1e-8\n",
    "\n",
    "#########\n",
    "# Flags #\n",
    "#########\n",
    "\n",
    "flags = tf.app.flags\n",
    "flags.DEFINE_string(\"file_pattern\", \"../data/ut-zap50k-images/*/*/*/*.jpg\", \"Pattern to find zap50k images\")\n",
    "flags.DEFINE_string(\"logdir\", \"./logdir\", \"Directory to save logs\")\n",
    "flags.DEFINE_string(\"sampledir\", None, \"Directory to save samples\")\n",
    "flags.DEFINE_boolean(\"classifier\", False, \"Use the discriminator for classification\")\n",
    "flags.DEFINE_boolean(\"kmeans\", False, \"Run kmeans of intermediate features\")\n",
    "flags.DEFINE_boolean(\"similarity\", False, \"Find most similar shoe\")\n",
    "flags.DEFINE_integer(\"batch_size\", 32, \"The size of batch images [32]\")\n",
    "flags.DEFINE_boolean(\"debug\", False, \"True if debug mode\")\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "if FLAGS.debug:\n",
    "    tf.set_random_seed(1)\n",
    "    np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################\n",
    "# Model settings #\n",
    "##################\n",
    "\n",
    "Z_DIM = 80\n",
    "C_DIM = 8\n",
    "C_COEFF = .05\n",
    "\n",
    "##########\n",
    "# Models #\n",
    "##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "            dataset = zap_data(FLAGS, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def generator(z, latent_c):\n",
    "depths = [32, 64, 64, 64, 64, 64, 3]\n",
    "sizes = zip(\n",
    "    np.linspace(4, IMAGE_SIZE['resized'][0], len(depths)).astype(np.int),\n",
    "    np.linspace(6, IMAGE_SIZE['resized'][1], len(depths)).astype(np.int))\n",
    "with slim.arg_scope([slim.conv2d_transpose],normalizer_fn=slim.batch_norm,kernel_size=3):\n",
    "    with tf.variable_scope(\"gen\"):\n",
    "        size = sizes.pop(0)\n",
    "        net = tf.concat(axis=1, values=[z, latent_c])\n",
    "        net = slim.fully_connected(net, depths[0] * size[0] * size[1])\n",
    "        net = tf.reshape(net, [-1, size[0], size[1], depths[0]])\n",
    "        for depth in depths[1:-1] + [None]:\n",
    "            net = tf.image.resize_images(net, sizes.pop(0),tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "            if depth:\n",
    "                net = slim.conv2d_transpose(net, depth)\n",
    "        net = slim.conv2d_transpose(net, depths[-1], activation_fn=tf.nn.tanh, stride=1, normalizer_fn=None)\n",
    "        tf.summary.image(\"gen\", net, max_outputs=8)\n",
    "#return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lrelu(x, leak=0.2, name=\"lrelu\"):\n",
    "    with tf.variable_scope(name):\n",
    "        f1 = 0.5 * (1 + leak)\n",
    "        f2 = 0.5 * (1 - leak)\n",
    "        return f1 * x + f2 * abs(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(z, latent_c):\n",
    "    depths = [32, 64, 64, 64, 64, 64, 3]\n",
    "    sizes = zip(\n",
    "        np.linspace(4, IMAGE_SIZE['resized'][0], len(depths)).astype(np.int),\n",
    "        np.linspace(6, IMAGE_SIZE['resized'][1], len(depths)).astype(np.int))\n",
    "    with slim.arg_scope([slim.conv2d_transpose],\n",
    "                        normalizer_fn=slim.batch_norm,\n",
    "                        kernel_size=3):\n",
    "        with tf.variable_scope(\"gen\"):\n",
    "            size = sizes.pop(0)\n",
    "            net = tf.concat(axis=1, values=[z, latent_c])\n",
    "            net = slim.fully_connected(net, depths[0] * size[0] * size[1])\n",
    "            net = tf.reshape(net, [-1, size[0], size[1], depths[0]])\n",
    "            for depth in depths[1:-1] + [None]:\n",
    "                net = tf.image.resize_images(\n",
    "                    net, sizes.pop(0),\n",
    "                    tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "                if depth:\n",
    "                    net = slim.conv2d_transpose(net, depth)\n",
    "            net = slim.conv2d_transpose(\n",
    "                net, depths[-1], activation_fn=tf.nn.tanh, stride=1, normalizer_fn=None)\n",
    "            tf.summary.image(\"gen\", net, max_outputs=8)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(input, reuse, dropout, int_feats=False, c_dim=None):\n",
    "    depths = [16 * 2**x for x in range(5)] + [16]\n",
    "    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
    "                        reuse=reuse,\n",
    "                        normalizer_fn=slim.batch_norm,\n",
    "                        activation_fn=lrelu):\n",
    "        with tf.variable_scope(\"discr\"):\n",
    "            net = input\n",
    "            for i, depth in enumerate(depths):\n",
    "                if i != 0:\n",
    "#                    net = slim.dropout(net, dropout, scope='dropout')\n",
    "                    net = slim.dropout(net, 0.5, scope='dropout')\n",
    "                if i % 2 == 0:\n",
    "                    net = slim.conv2d(\n",
    "                        net, depth, kernel_size=3, stride=2, scope='conv%d' % i)\n",
    "                else:\n",
    "                    net = slim.conv2d(\n",
    "                        net, depth, kernel_size=3, scope='conv%d' % i)\n",
    "            net = slim.flatten(net)\n",
    "            if int_feats:\n",
    "                return net\n",
    "            else:\n",
    "                d_net = slim.fully_connected(\n",
    "                    net, 1, activation_fn=tf.nn.sigmoid, normalizer_fn=None, scope='out')\n",
    "    if c_dim:\n",
    "        with tf.variable_scope('latent_c'):\n",
    "            q_net = slim.fully_connected(\n",
    "                net, c_dim, activation_fn=tf.nn.tanh, scope='out')\n",
    "        return d_net, q_net\n",
    "    return d_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(d_model, g_model, dg_model, q_model, latent_c):\n",
    "    t_vars = tf.trainable_variables()\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "    # Latent_C\n",
    "    q_loss = tf.reduce_sum(0.5 * tf.square(latent_c - q_model)) * C_COEFF\n",
    "\n",
    "    # Discriminator\n",
    "    d_loss = -tf.reduce_mean(tf.log(d_model + TINY) + tf.log(1. - dg_model + TINY))\n",
    "    tf.summary.scalar('d_loss', d_loss)\n",
    "    d_trainer = tf.train.AdamOptimizer(.0002, beta1=.5).minimize(\n",
    "        d_loss + q_loss,\n",
    "        global_step=global_step,\n",
    "        var_list=[v for v in t_vars if 'discr/' in v.name or 'latent_c/' in v.name])\n",
    "\n",
    "    # Generator\n",
    "    g_loss = -tf.reduce_mean(tf.log(dg_model + TINY))\n",
    "    tf.summary.scalar('g_loss', g_loss)\n",
    "    g_trainer = tf.train.AdamOptimizer(.001, beta1=.5).minimize(\n",
    "        g_loss + q_loss,\n",
    "        var_list=[v for v in t_vars if 'gen/' in v.name or 'latent_c/' in v.name])\n",
    "\n",
    "    return d_trainer, d_loss, g_trainer, g_loss, global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#######\n",
    "# GAN #\n",
    "#######\n",
    "\n",
    "\n",
    "def gan(dataset, sess):\n",
    "    # Model\n",
    "    x = tf.placeholder(tf.float32, shape=[\n",
    "        None, IMAGE_SIZE['resized'][0], IMAGE_SIZE['resized'][1], 3])\n",
    "    dropout = tf.placeholder(tf.float32)\n",
    "    d_model = discriminator(x, reuse=False, dropout=dropout)\n",
    "\n",
    "    z = tf.placeholder(tf.float32, shape=[None, Z_DIM])\n",
    "    latent_c = tf.placeholder(shape=[None, C_DIM], dtype=tf.float32)\n",
    "    g_model = generator(z, latent_c)\n",
    "    dg_model, q_model = discriminator(\n",
    "        g_model, reuse=True, dropout=dropout, c_dim=C_DIM)\n",
    "\n",
    "    d_trainer, d_loss, g_trainer, g_loss, global_step = loss(\n",
    "        d_model, g_model, dg_model, q_model, latent_c)\n",
    "\n",
    "    # Stats\n",
    "    t_vars = tf.trainable_variables()\n",
    "    count_params(t_vars, ['discr/', 'gen/', 'latent_c/'])\n",
    "    # for v in t_vars:\n",
    "    # tf.histogram_summary(v.name, v)\n",
    "\n",
    "    # Init\n",
    "    summary = tf.summary.merge_all()\n",
    "    summary_writer = tf.summary.FileWriter(FLAGS.logdir, sess.graph)\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    # Saver\n",
    "    saver = tf.train.Saver(max_to_keep=10)\n",
    "    checkpoint = tf.train.latest_checkpoint(FLAGS.logdir)\n",
    "    if checkpoint:\n",
    "        print('Restoring from', checkpoint)\n",
    "        saver.restore(sess, checkpoint)\n",
    "\n",
    "    # Dataset queue\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    tf.train.start_queue_runners(sess=sess)\n",
    "\n",
    "    # Training loop\n",
    "    for step in range(global_step.eval(), 1 if FLAGS.debug else int(1e6)):\n",
    "        z_batch = np.random.uniform(-1, 1, [FLAGS.batch_size, Z_DIM]).astype(np.float32)\n",
    "        c_batch = np.random.uniform(-1, 1, [FLAGS.batch_size, C_DIM])\n",
    "        images, _ = sess.run(dataset['batch'])\n",
    "        feed_dict = {z: z_batch, latent_c: c_batch, x: images, dropout: .5, }\n",
    "\n",
    "        # Update discriminator\n",
    "        start = time.time()\n",
    "        _, d_loss_val = sess.run([d_trainer, d_loss], feed_dict=feed_dict)\n",
    "        d_time = time.time() - start\n",
    "\n",
    "        # Update generator\n",
    "        start = time.time()\n",
    "        _, g_loss_val, summary_str = sess.run([g_trainer, g_loss, summary], feed_dict=feed_dict)\n",
    "        g_time = time.time() - start\n",
    "\n",
    "        # Log details\n",
    "        if step % 10 == 0 or FLAGS.debug:\n",
    "            print(\"[%s, %s] Disc loss: %.3f (%.2fs), Gen Loss: %.3f (%.2fs)\" %\n",
    "                  (step, step * FLAGS.batch_size / dataset['size'], d_loss_val, d_time, g_loss_val, g_time, ))\n",
    "            summary_writer.add_summary(summary_str, global_step.eval())\n",
    "\n",
    "        # Early stopping\n",
    "        if np.isnan(g_loss_val) or np.isnan(d_loss_val):\n",
    "            print('Early stopping', g_loss_val, d_loss_val)\n",
    "            break\n",
    "\n",
    "        # save model\n",
    "        if step % 1000 == 0 and not FLAGS.debug:\n",
    "            print('Saving')\n",
    "            checkpoint_file = os.path.join(FLAGS.logdir, 'checkpoint')\n",
    "            saver.save(sess, checkpoint_file, global_step=global_step)\n",
    "\n",
    "    # Finish off the filename queue coordinator.\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########\n",
    "# Sample #\n",
    "##########\n",
    "\n",
    "def sample(FLAGS, sess):\n",
    "    # Model\n",
    "    z = tf.placeholder(tf.float32, shape=[None, Z_DIM])\n",
    "    latent_c = tf.placeholder(shape=[None, C_DIM], dtype=tf.float32)\n",
    "    g_model = generator(z, latent_c)\n",
    "\n",
    "    # Restore\n",
    "    saver = tf.train.Saver()\n",
    "    checkpoints = [x for x in glob(FLAGS.logdir + '/checkpoint-*') if 'meta' not in x]\n",
    "    checkpoints = [tf.train.latest_checkpoint(FLAGS.logdir)]\n",
    "    for checkpoint in checkpoints:\n",
    "        saver.restore(sess, checkpoint)\n",
    "\n",
    "        # Save samples\n",
    "        output = \"samples/%s.png\" % os.path.basename(checkpoint)\n",
    "        samples = 144\n",
    "        width = math.sqrt(samples)\n",
    "\n",
    "        # Input\n",
    "        z_batch = np.random.uniform(-1.0, 1.0, size=[samples, Z_DIM]).astype(np.float32)\n",
    "        c_batch = np.zeros((samples, C_DIM))\n",
    "        if 0:\n",
    "            for i in range(8):\n",
    "                c_batch[i * width:(i + 1) * width, i] = np.linspace(-1, 1, width)\n",
    "        else:\n",
    "            c_batch[:, 0] = np.tile(np.linspace(-1, 1, width), width)\n",
    "            c_batch[:, 1] = np.repeat(np.linspace(-1, 1, width), width)\n",
    "\n",
    "        # Run and save\n",
    "        images = sess.run(g_model, feed_dict={z: z_batch, latent_c: c_batch})\n",
    "        images = np.reshape(\n",
    "            images, [samples, IMAGE_SIZE['resized'][0], IMAGE_SIZE['resized'][1], 3])\n",
    "        images = (images + 1.) / 2.\n",
    "        scipy.misc.imsave(output, merge(images, [int(width)] * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############\n",
    "# Similarity #\n",
    "##############\n",
    "\n",
    "@cache_result\n",
    "def export_intermediate(FLAGS, sess, dataset):\n",
    "    # Models\n",
    "    x = tf.placeholder(tf.float32, shape=[\n",
    "        None, IMAGE_SIZE['resized'][0], IMAGE_SIZE['resized'][1], 3])\n",
    "    dropout = tf.placeholder(tf.float32)\n",
    "    feat_model = discriminator(x, reuse=False, dropout=dropout, int_feats=True)\n",
    "\n",
    "    # Init\n",
    "    init_op = tf.group(tf.global_variables_initializer(),\n",
    "                       tf.local_variables_initializer())\n",
    "    sess.run(init_op)\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    # Restore\n",
    "    saver = tf.train.Saver()\n",
    "    checkpoint = tf.train.latest_checkpoint(FLAGS.logdir)\n",
    "    saver.restore(sess, checkpoint)\n",
    "\n",
    "    # Run\n",
    "    all_features = np.zeros((dataset['size'], feat_model.get_shape()[1]))\n",
    "    all_paths = []\n",
    "    for i in itertools.count():\n",
    "        try:\n",
    "            images, paths = sess.run(dataset['batch'])\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "        if i % 10 == 0:\n",
    "            print(i * FLAGS.batch_size, dataset['size'])\n",
    "        im_features = sess.run(feat_model, feed_dict={x: images, dropout: 1, })\n",
    "        all_features[FLAGS.batch_size * i:FLAGS.batch_size * i + im_features.shape[0]] = im_features\n",
    "        all_paths += list(paths)\n",
    "\n",
    "    # Finish off the filename queue coordinator.\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "\n",
    "    return all_features, all_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similarity(FLAGS, sess, all_features, all_paths):\n",
    "    def select_images(distances):\n",
    "        indices = np.argsort(distances)\n",
    "        images = []\n",
    "        size = 40\n",
    "        for i in range(size):\n",
    "            images += [dict(path=all_paths[indices[i]],\n",
    "                            index=indices[i],\n",
    "                            distance=distances[indices[i]])]\n",
    "        return images\n",
    "\n",
    "    # Distance\n",
    "    x1 = tf.placeholder(tf.float32, shape=[None, all_features.shape[1]])\n",
    "    x2 = tf.placeholder(tf.float32, shape=[None, all_features.shape[1]])\n",
    "    l2diff = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(x1, x2)), axis=1))\n",
    "\n",
    "    # Init\n",
    "    init_op = tf.group(tf.global_variables_initializer(),\n",
    "                       tf.local_variables_initializer())\n",
    "    sess.run(init_op)\n",
    "\n",
    "    #\n",
    "    clip = 1e-3\n",
    "    np.clip(all_features, -clip, clip, all_features)\n",
    "\n",
    "    # Get distances\n",
    "    result = []\n",
    "    bs = 100\n",
    "    needles = [randint(0, all_features.shape[0]) for x in range(10)]\n",
    "    for needle in needles:\n",
    "        item_block = np.reshape(np.tile(all_features[needle], bs), [bs, -1])\n",
    "        distances = np.zeros(all_features.shape[0])\n",
    "        for i in range(0, all_features.shape[0], bs):\n",
    "            if i + bs > all_features.shape[0]:\n",
    "                bs = all_features.shape[0] - i\n",
    "            distances[i:i + bs] = sess.run(\n",
    "                l2diff, feed_dict={x1: item_block[:bs], x2: all_features[i:i + bs]})\n",
    "\n",
    "        # Pick best matches\n",
    "        result += [select_images(distances)]\n",
    "\n",
    "    with open('./logdir/data.json', 'w') as f:\n",
    "        json.dump(dict(data=result), f)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "# Main #\n",
    "########\n",
    "\n",
    "def main(_):\n",
    "    if not tf.gfile.Exists(FLAGS.logdir):\n",
    "        tf.gfile.MakeDirs(FLAGS.logdir)\n",
    "    if FLAGS.sampledir and not tf.gfile.Exists(FLAGS.sampledir):\n",
    "        tf.gfile.MakeDirs(FLAGS.sampledir)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        if FLAGS.sampledir:\n",
    "            sample(FLAGS, sess)\n",
    "        elif FLAGS.similarity:\n",
    "            dataset = zap_data(FLAGS, False)\n",
    "            all_features, all_paths = export_intermediate(FLAGS, sess, dataset)\n",
    "            similarity(FLAGS, sess, all_features, all_paths)\n",
    "        else:\n",
    "            dataset = zap_data(FLAGS, True)\n",
    "            gan(dataset, sess)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
